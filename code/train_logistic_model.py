from pyspark.sql import SparkSession
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator


spark = SparkSession.builder.appName("Logistic Regression with Spark") \
    .config("spark.executor.memory", "10g") \
    .config("spark.memory.fraction", "0.6") \
    .config("spark.memory.storageFraction", "0.5") \
    .config("spark.driver.memory", "8g") \
    .config("spark.executor.cores", "6") \
    .config("spark.kryoserializer.buffer.max", "1g") \
    .getOrCreate()

train_data = spark.read.parquet("../data/clean_train_data.parquet_17")
train_data.show()
test_data = spark.read.parquet("../data/clean_test_data.parquet")
data_for_local_test = spark.read.parquet("../data/data_for_local_test.parquet")

logistic_model = LogisticRegression(featuresCol="features", labelCol="HasDetections")
logistic_model = logistic_model.fit(train_data)
predictions = logistic_model.transform(test_data)

# save logistic model
logistic_model.write().save("../models/logistic_model")

predictions = predictions.select("MachineIdentifier", "prediction")

final_predictions = predictions.withColumnRenamed("binary_prediction", "HasDetections")

final_predictions.show()

final_predictions.coalesce(1).write.csv("../submissions/logistic_model_prediction.csv", header=True, mode="overwrite")



##### test logistic model performance
local_test_predictions = logistic_model.transform(data_for_local_test)

# Calculate ROC
evaluator = BinaryClassificationEvaluator(labelCol="HasDetections", rawPredictionCol="prediction", metricName="areaUnderROC")
auc = evaluator.evaluate(local_test_predictions)
print("Test Area Under ROC: " + str(auc))


# Calculate Accuracy
accuracy_evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="accuracy")
accuracy = accuracy_evaluator.evaluate(local_test_predictions)
print("Accuracy: %g" % accuracy)


# Calculate F1 Score
f1_evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="prediction", metricName="f1")
f1_score = f1_evaluator.evaluate(local_test_predictions)
print("F1 Score: %g" % f1_score)

spark.stop()