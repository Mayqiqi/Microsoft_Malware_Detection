from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
from pyspark.sql.functions import when
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler
from pyspark.sql.types import StringType, DoubleType, IntegerType
from pyspark.sql.functions import col, count, when
from pyspark.ml.regression import LinearRegression
import matplotlib.pyplot as plt

# initiate Spark
spark = SparkSession.builder.appName("MalwarePrediction") \
    .config("spark.executor.memory", "6g") \
    .config("spark.memory.fraction", "0.6") \
    .config("spark.memory.storageFraction", "0.5") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# load data
train_data = spark.read.csv("../data/train.csv", header=True, inferSchema=True)
test_data = spark.read.csv("../data/test.csv", header=True, inferSchema=True)

train_data.show()



train_data_total_count = train_data.count()

null_percentage = train_data.select(*[(count(when(col(c).isNull(), c)) / train_data_total_count).alias(c) for c in train_data.columns])
null_data = null_percentage.collect()[0].asDict()


columns = list(null_data.keys())
values = list(null_data.values())

plt.figure(figsize=(15, 8))
plt.bar(null_data.keys(), null_data.values(), color='blue')

# set x
plt.xlabel('Columns')
plt.ylabel('Null Value Percentage')
plt.title('Percentage of Null Values in Each Column')

ticks_positions = list(range(len(null_data)))
ticks_labels = [col if perc > 0.6 else "" for col, perc in null_data.items()]

# set x
plt.xticks(ticks_positions, ticks_labels, rotation=90)  # rotate 90 degree for better readability

# show the graph
plt.tight_layout()  # set tight layout

# save graph

plt.savefig('../images/null_values_percentage.png')
plt.show()
plt.close()

columns_to_drop = [c for c, p in null_data.items() if p > 0.6]
print(columns_to_drop)

train_data_drop_nulls = train_data.drop(*columns_to_drop)
test_data_drop_nulls = test_data.drop(*columns_to_drop)

train_data_drop_nulls.write.parquet("../data/no_null_train_data.parquet")
test_data_drop_nulls.write.parquet("../data/no_null_test_data.parquet")