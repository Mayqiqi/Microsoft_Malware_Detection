from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml import Pipeline
import numpy

spark = SparkSession.builder \
    .appName("Linear Regression with Spark") \
    .config("spark.executor.memory", "6g") \
    .config("spark.memory.fraction", "0.6") \
    .config("spark.memory.storageFraction", "0.5") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()


train_data = spark.read.parquet("../data/clean_train_data.parquet")
test_data = spark.read.parquet("../data/clean_test_data.parquet")


lr = LinearRegression(featuresCol="features", labelCol="HasDetections")
lr_model = lr.fit(train_data)
predictions = lr_model.transform(test_data)


lr_model.write().save("../models/linear_model")


predictions = predictions.select("MachineIdentifier", "prediction")
binary_predictions = predictions.withColumn("binary_prediction", when(col("prediction") >= 0.5, 1).otherwise(0))
final_predictions = binary_predictions.withColumnRenamed("binary_prediction", "HasDetections")
final_predictions = final_predictions.select("MachineIdentifier", "HasDetections")
final_predictions.show()

final_predictions.coalesce(1).write.csv("../submissions/linear_model_prediction.csv", header=True, mode="overwrite")



# binary_evaluator = BinaryClassificationEvaluator(labelCol="HasDetections", rawPredictionCol="prediction", metricName="areaUnderROC")
# auc = binary_evaluator.evaluate(binary_predictions)
# print("Area under ROC:", auc)
#
# # 使用MulticlassClassificationEvaluator评估准确率、召回率、精确率和F1分数
# multi_evaluator = MulticlassClassificationEvaluator(labelCol="HasDetections", predictionCol="binary_prediction")
# accuracy = multi_evaluator.evaluate(binary_predictions, {multi_evaluator.metricName: "accuracy"})
# precision = multi_evaluator.evaluate(binary_predictions, {multi_evaluator.metricName: "weightedPrecision"})
# recall = multi_evaluator.evaluate(binary_predictions, {multi_evaluator.metricName: "weightedRecall"})
# f1 = multi_evaluator.evaluate(binary_predictions, {multi_evaluator.metricName: "f1"})
#
# print("Accuracy:", accuracy)
# print("Precision:", precision)
# print("Recall:", recall)
# print("F1 Score:", f1)


spark.stop()